name: BUILD
on: 
  workflow_dispatch:

jobs:
  Build:
    runs-on: ubuntu-latest
    steps:
    
      - uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      - uses: actions/setup-java@v1
        with:
          java-version: '11'
      - uses: vemonet/setup-spark@v1
        with:
          spark-version: '3.1.2'
          hadoop-version: '3.2'

      - name: Checkout
        uses: actions/checkout@v2

#       - name: Setup gcloud
#         uses: google-github-actions/setup-gcloud@master
#         with:
#           project_id: dbk-appops-nonprod
#           service_account_key: ${{ secrets.JENKINS_NONPROD_GCP_SA }}
      # Installs code coverage tool (only for tests)
      - name: pip installs for tests
        run: |
          pip install coverage
          pip install pyspark-test

      - name: Run tests with coverage
        run: |
          coverage run --source=dataproc -m unittest discover
          
      - name: Report
        run: |
          coverage report --show-missing --sort=Cover --skip-empty

      - name: Egg
        run: |
          python setup.py bdist_egg
          unzip -l dist/*.egg
          unzip -c dist/*.egg EGG-INFO/PKG-INFO

#       - name: Upload to GCS
#         id: upload-files
#         uses: google-github-actions/upload-cloud-storage@main
#         with:
#           path: dist/channel_intelligence-0.0.1-py3.9.egg
#           destination: dbk-minerva-cug01-prd-us-dataproc-temp
